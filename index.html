<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Di Hu</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Home</div>
<div class="menu-item"><a href="index.html" class="current">About&nbsp;me</a></div>
<div class="menu-category">Research</div>
<div class="menu-item"><a href="https://gewu-lab.github.io/">GeWu&nbsp;Lab</a></div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
<div class="menu-item"><a href="dataset.html">Dataset</a></div>
<div class="menu-item"><a href="project.html">Projects</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Di Hu</h1>
</div>
<table class="imgtable"><tr><td>
<img src="ims/self_5.png" alt="alt text" height="280px" />&nbsp;</td>
<td align="left"><p>Di Hu  胡 迪<br />
Associate Professor (tenure-track) <br /> 
<a href="http://ai.ruc.edu.cn/">Gaoling School of Artificial Intelligence</a>, <a href="https://www.ruc.edu.cn/">Renmin University of China</a> <br />
Contact: dihu[at]ruc.edu.cn, <a href="https://github.com/DTaoo">Github</a>, <a href="https://www.zhihu.com/column/c_1280978171983507456">Zhihu</a>, <a href="papers/DTao_Resume.pdf">Resume</a></p>
</td></tr></table>
<h2>Short Bio </h2>
<p>I'm tenure-track faculty at <a href="http://ai.ruc.edu.cn/">Gaoling School of Artificial Intelligence</a>, <a href="https://www.ruc.edu.cn/">Renmin University of China</a>. Before that, I was previously a research scientist at <a href="http://research.baidu.com/">Baidu Research</a>. I obtained the PhD degree from <a href="http://en.nwpu.edu.cn">Northwestern Polytechnical University</a> in 2019, supervised by <a href="https://scholar.google.com.hk/citations?user=ahUibskAAAAJ&amp;hl=zh-CN">Xuelong Li</a>, <a href="https://scholar.google.com.hk/citations?user=FRyuu2IAAAAJ&amp;hl=zh-CN">Xiaoqiang Lu</a>, and <a href="http://www.escience.cn/people/fpnie/index.html">Feiping Nie</a>. What I met, told and learned in the Center for OPTical IMagery Analysis and Learning (OPTIMAL) led by Prof. <a href="https://scholar.google.com.hk/citations?user=ahUibskAAAAJ&amp;hl=zh-CN">Xuelong Li</a> have undoubtedly made great impacts on me. Great Thanks!</p>
<p>Currently, I'm leading the <a href="https://gewu-lab.github.io/">GeWu Lab</a> and exploring how to understand and interact with the world via the natural multimodal messages. I'm strongly convinced that the pervasive and free correspondence between multimodal messages can provide sufficient information for perceiving, learning and understanding environment, even the agent itself, which promisingly makes multimodal perception become one of the key to achieve machine intelligence.</p>
<p><b>I'm always looking for self-motivated Ph.D and Master students, also including visiting students.</b> If you're interested, please drop me your email!</p>
<h2>Recent News</h2>
<ul>
<li><p>[2024] For more recent news, please visit our <a href="https://gewu-lab.github.io/">GeWu-Lab Homepage</a>.</p>
</li>
<li><p>[22-1-2025] One paper accepted by ICLR. Congrats to Ruoxuan and Wenke!</p>
</li>
<li><p>[17-09-2024] One paper accepted by T-PAMI. Congrats to Yake and Henghui!</p>
</li>
<li><p>[05-09-2024] Two papers accepted by CoRL 2024 with an <b>Oral</b>. Congrats to Jingxian, Wenke and Ruoxuan!</p>
</li>
<li><p>[16-07-2024] Two papers accepted by ACMMM 2024 with an <b>Oral</b>. Congrats to Guangyao and Peiwen!</p>
</li>
<li><p>[30-06-2024] Four papers accepted by ECCV 2024. Congrats to Yaoting, Peiwen, Juncheng and Yake!</p>
</li>
<li><p>[30-06-2024] One paper accepted by IROS 2024. Congrats to Xincheng and Wenke!</p>
</li>
<li><p>[26-05-2024] One paper accepted by TOMM 2024. Congrats to Wenxuan!</p>
</li>
<li><p>[14-05-2024] One paper accepted by RSS 2024. Congrats to Wenke!</p>
</li>
<li><p>[06-05-2024] One paper accepted by ICML 2024. Congrats to Yake!</p>
</li>
<li><p>[24-04-2024] Guangyao passed his Ph.D. thesis defense. Congrats to him!</p>
</li>
<li><p>[28-02-2024] One paper accepted by CVPR 2024. Congrats to Yake!</p>
</li>
<li><p>[01-02-2024] One paper accepted by ICRA 2024. Congrats to Wenke and Xincheng!</p>
</li>
<li><p>[16-01-2024] One paper accepted by ICLR 2024. Congrats to Zequn!</p>
</li>
<li><p>[11-12-2023] Yake has begun her visiting to CMU. Best wishes to her!</p>
</li>
<li><p>[10-12-2023] Two papers accepted by AAAI 2024. Congrats to Yaoting and Guangyao!</p>
</li>
<li><p>[01-11-2023] Invited to serve as the Workshop Chair @<a href="https://ceii.asia/index.html">CEII2023</a>.</p>
</li>
<li><p>[20-10-2023] One paper accepted by Pattern Recognition. Congrats to Zequn!</p>
</li>
<li><p>[26-07-2023] Two papers accepted by ACMMM. Congrats to Guangyao, Wenxuan, Yixin and Wenke!</p>
</li>
<li><p>[14-07-2023] One paper accepted by ICCV 2023. Congrats to Andong!</p>
</li>
<li><p>[18-05-2023] One paper accepted by Interspeech 2023. Congrats to Guangyao!</p>
</li>
<li><p>[14-03-2023] Dr. Di Hu was awarded the 2022 WuWenJun AI Excellent Young Scientist Award!</p>
</li>
<li><p>[13-03-2023] One paper accepted by ICME 2023. Congrats to Wenke!</p>
</li>
<li><p>[16-02-2023] One paper accepted by ICASSP 2023. Congrats to Ruize and Ruoxuan!</p>
</li>
<li><p>[14-02-2023] Dr. Di Hu was invited as the Senior PC Member in IJCAI 2023</p>
</li>
<li><p>[25-11-2022] Our work on self-supervised earth observation has been accepted by IJAEOG! Congrats to Konrad!</p>
</li>
<li><p>[11-10-2022] Two papers accepted by WACV 2022! Congrats to Xinchi, DongZhan!</p>
</li>
<li><p>[27-09-2022] Dr. Di Hu gave a talk @ByteDance AI Lab, and had a fantastic discussion with the researchers!</p>
</li>
<li><p>[25-08-2022] We wrote an article about recent advances in audio-visual learning, Please check <a href="https://gewu-lab.github.io/audio-visual-learning/">here</a>!  </p>
</li>
<li><p>[24-08-2022] Dr. Di Hu gave a talk @Valse 2022 in Tian Jin. Please find slides <a href="materials/VALSE-2022-DiHu.pdf">here</a>!</p>
</li>
<li><p>[12-07-2022] Dr. Di Hu was invited as the Senior PC Member in AAAI 2023.</p>
</li>
<li><p>[11-07-2022] Dr. Di Hu will chair the &ldquo;Audiovisual Enhancement&rdquo; session in ICME 2022.</p>
</li>
<li><p>[11-03-2022] One paper accepted by TMM. Congrats to all the co-authors!</p>
</li>
<li><p>[02-03-2022] Two papers accepted by CVPR 2022, both are <b>ORAL</b> presentation! Congrats to Guangyao, Xiaokang, Yake and Andong!</p>
</li>
</ul>
<h2>Research Interests </h2>
<ul>
<li><p>Multimodal Machine Learning</p>
</li>
<li><p>Machine Multimodal Perception</p>
</li>
</ul>
<h2>Course </h2>
<ul>
<li><p>Artificial Intelligence and Python Programming (Undergraduate, Spring, 2021 and 2022)</p>
</li>
<li><p>Pattern Recognition and Computer Vision (Graduate, Spring, 2021 and 2022)</p>
</li>
</ul>
<h2>Services and Experiences</h2>
<ul>
<li><p>Senior PC Member and Session Chair: AAAI 2023-2025, IJCAI 2023-2024, ICME 2022</p>
</li>
<li><p>PC Member: ICML 2021-2024, NeurIPS 2020-2024, CVPR 2018 2020-2024, ICCV 2019-2023, ECCV 2020, AAAI 2018 2020-2023, ACCV 2018 2020, WACV 2021</p>
</li>
<li><p>Co-organizer: ICDM 2019 Tutorial on <a href="https://baiduautodl.com/">Automated Deep Learning: Theory, Algorithms, Platforms, and Applications</a></p>
</li>
<li><p>Co-organizer: WACV2021 Tutorial on <a href="https://echo0409.github.io/Audio-Visual-Scene-Understanding/">Audio-visual Scene Understanding</a></p>
</li>
<li><p>Co-organizer: CVPR2021 Tutorial on <a href="https://audio-visual-scene-understanding.github.io/">Audio-visual Scene Understanding</a></p>
</li>
<li><p>CVPR 2019 Doctoral Consortium: Had a memorable discussion with <a href="https://www.cs.cmu.edu/~morency/">Prof. Morency</a></p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2025-02-07 18:11:03 CST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
